# -*- coding: utf-8 -*-
import os
import math
import tempfile
import subprocess
import streamlit as st
import openai # pip install openai


# Use faster-whisper for word-level timestamps
# pip install faster-whisper
from faster_whisper import WhisperModel

# Use pydub for audio extraction
# pip install pydub
from pydub import AudioSegment

# ----------------------------
# Constants and UI Configuration
# ----------------------------
MODEL_SIZES = ["tiny", "base", "small", "medium", "large-v2", "large-v3"]
DEFAULT_MODEL_SIZE = "base" # Consider "small" or "medium" for better quality if resources allow
MAX_CHARS_PER_SUB = 60 # Maximum characters per subtitle line
DEFAULT_BUCKET_SECONDS = 5 # Default duration for subtitle buckets (maximum duration)
DEFAULT_MIN_SUBTITLE_DURATION = 0.5 # Default minimum duration for a subtitle entry

# Ensure FFmpeg is accessible (it's installed via packages.txt on Streamlit Cloud)
# AudioSegment.converter = "ffmpeg" # Pydub finds it automatically if in PATH

# ----------------------------
# Helper Functions
# ----------------------------
def hhmmss_ms(seconds: float) -> str:
"""
   Converts seconds to SRT time format HH:MM:SS,mmm
   """
ms = int(round((seconds - int(seconds)) * 1000))
total_seconds = int(seconds)
h = total_seconds // 3600
m = (total_seconds % 3600) // 60
s = total_seconds % 60
return f"{h:02}:{m:02}:{s:02},{ms:03}"

@st.cache_resource
def load_model(model_size: str):
"""
   Loads the Faster-Whisper model. Uses cache for efficiency.
   """
st.info(f"Loading Faster-Whisper model: {model_size}. This may take a while for larger models.")
# compute_type="int8" is generally best for CPU to avoid ValueError on Streamlit Cloud
return WhisperModel(model_size, device="cpu", compute_type="int8")

@st.cache_resource(hash_funcs={WhisperModel: lambda _: None}) # Exclude WhisperModel from hashing as it's unhashable
def transcribe_words(_model: WhisperModel, audio_path: str, lang: str = None):
"""
   Transcribes audio using Faster-Whisper with word-level timestamps.
   lang=None will auto-detect the language and transcribe in that language.
   """
st.info(f"Transcribing audio with word-level timestamps. Language set to: {lang if lang else 'Auto-detect'}")
segments, info = _model.transcribe(
audio_path,
word_timestamps=True, 
language=lang, # Set to None for auto-detection
beam_size=7 
)
words = []
for segment in segments:
for word in segment.words:
words.append(word)

detected_language = info.language
st.info(f"Detected language: {detected_language.upper()}")
return words, detected_language

def bucket_words_by_duration(words: list, bucket_seconds: int = 5, max_chars_per_sub: int = 60, min_subtitle_duration: float = 0.5) -> str:
"""
   Creates SRT content by segmenting based on natural pauses (punctuation)
   and then falling back to duration/character limits if needed.
   Ensures each subtitle meets a minimum display duration.
   """
srt_content_blocks = [] # To store list of dicts for each subtitle block
current_words_in_segment = []

def ends_with_punctuation(word_text):
# Handles common English and Chinese punctuation
return word_text.endswith(('.', '?', '!', ',', '„ÄÇ', 'Ôºü', 'ÔºÅ', 'Ôºå'))

for i, word in enumerate(words):
word_text = word.word 
current_words_in_segment.append(word)

segment_text = " ".join([w.word for w in current_words_in_segment]).strip()

segment_duration = 0.0
if current_words_in_segment:
segment_duration = current_words_in_segment[-1].end - current_words_in_segment[0].start

segment_char_length = len(segment_text)

punctuation_break = False
if ends_with_punctuation(word_text):
if word_text.endswith(('.', '?', '!', '„ÄÇ', 'Ôºü', 'ÔºÅ')):
punctuation_break = True
elif word_text.endswith((',', 'Ôºå')):
if segment_duration >= (min_subtitle_duration + 0.5) or segment_char_length >= (max_chars_per_sub / 2):
punctuation_break = True

if i < len(words) - 1:
next_word_start = words[i+1].start
if (next_word_start - word.end) > 0.4: # Significant pause
punctuation_break = True

duration_exceeded = segment_duration >= bucket_seconds
chars_exceeded = segment_char_length >= max_chars_per_sub
is_last_word = (i == len(words) - 1)

should_finalize = False
if is_last_word:
should_finalize = True
elif punctuation_break and segment_duration >= min_subtitle_duration:
should_finalize = True
elif duration_exceeded and segment_duration >= min_subtitle_duration:
should_finalize = True
elif chars_exceeded and segment_duration >= min_subtitle_duration:
should_finalize = True

if should_finalize and segment_text.strip() == "" and not is_last_word:
should_finalize = False

if should_finalize:
if current_words_in_segment:
start_time = current_words_in_segment[0].start
end_time = current_words_in_segment[-1].end + 0.02 # Add 20ms offset for smoother transition

if (end_time - start_time) < min_subtitle_duration:
end_time = start_time + min_subtitle_duration

subtitle_text = " ".join([w.word.strip() for w in current_words_in_segment]).strip()

# Line breaking for readability
if len(subtitle_text) > max_chars_per_sub:
break_point = -1
for k in range(min(len(subtitle_text) -1, max_chars_per_sub -1), -1, -1):
if subtitle_text[k] in (',', '.', '?', '!', '„ÄÇ', 'Ôºü', 'ÔºÅ', 'Ôºå'):
break_point = k + 1
break
if break_point == -1:
break_point = subtitle_text.rfind(' ', 0, max_chars_per_sub)
if break_point == -1:
break_point = max_chars_per_sub

line1 = subtitle_text[:break_point].strip()
line2 = subtitle_text[break_point:].strip()
subtitle_text_final = f"{line1}\n{line2}"
else:
subtitle_text_final = subtitle_text

if subtitle_text_final:
srt_content_blocks.append({
"id": subtitle_idx,
"start": start_time,
"end": end_time,
"text": subtitle_text_final
})
subtitle_idx += 1

current_words_in_segment = [] # Reset for next segment

# Final check for any remaining words
if current_words_in_segment:
start_time = current_words_in_segment[0].start
end_time = current_words_in_segment[-1].end + 0.02
if (end_time - start_time) < min_subtitle_duration:
end_time = start_time + min_subtitle_duration

final_segment_text = " ".join([w.word.strip() for w in current_words_in_segment]).strip()
if len(final_segment_text) > max_chars_per_sub:
break_point = -1
for k in range(min(len(final_segment_text) -1, max_chars_per_sub -1), -1, -1):
if final_segment_text[k] in (',', '.', '?', '!', '„ÄÇ', 'Ôºü', 'ÔºÅ', 'Ôºå'):
break_point = k + 1
break
if break_point == -1:
break_point = final_segment_text.rfind(' ', 0, max_chars_per_sub)
if break_point == -1:
break_point = max_chars_per_sub

line1 = final_segment_text[:break_point].strip()
line2 = final_segment_text[break_point:].strip()
final_subtitle_text = f"{line1}\n{line2}"
else:
final_subtitle_text = final_segment_text

if final_subtitle_text:
srt_content_blocks.append({
"id": subtitle_idx,
"start": start_time,
"end": end_time,
"text": final_subtitle_text
})

return srt_content_blocks # Return list of dicts for easier translation

def assemble_srt_text(srt_blocks):
"""Assembles SRT text from a list of subtitle blocks."""
srt_output = []
for block in srt_blocks:
srt_output.append(str(block["id"]))
srt_output.append(f"{hhmmss_ms(block['start'])} --> {hhmmss_ms(block['end'])}")
srt_output.append(block["text"])
srt_output.append("")
return "\n".join(srt_output)


# New Translation Function using OpenAI API
@st.cache_data(show_spinner=False) # Cache translated results
def translate_text_openai(text_to_translate: str, target_language: str = "English", source_language: str = None) -> str:
"""
   Translates text using OpenAI's Chat Completion API.
   """
if not st.secrets.get("openai_api_key"):
st.warning("OpenAI API key not found in Streamlit secrets. Translation will be skipped.")
return text_to_translate

try:
client = openai.OpenAI(api_key=st.secrets["openai_api_key"])

system_prompt = f"You are a highly accurate translator. Translate the given text into {target_language}."
if source_language:
system_prompt += f" The original language is {source_language}."

response = client.chat.completions.create(
model="gpt-3.5-turbo", # You can try "gpt-4" if you have access and need higher quality (higher cost)
messages=[
{"role": "system", "content": system_prompt},
{"role": "user", "content": text_to_translate}
],
temperature=0.7, # Lower for more literal translation, higher for more creative
max_tokens=500 # Adjust based on expected output length
)
translated_content = response.choices[0].message.content.strip()
return translated_content
except openai.APICallError as e:
st.error(f"OpenAI API Error: {e.response.status_code} - {e.response.json()['error']['message']}")
return f"[Translation Failed: {e.response.json()['error']['message']}] {text_to_translate}"
except Exception as e:
st.error(f"An unexpected error occurred during OpenAI translation: {e}")
return f"[Translation Error] {text_to_translate}"

def ensure_ffmpeg_access():
"""
   Ensures the ffmpeg executable is available in the system PATH.
   It will be installed via packages.txt during deployment.
   """
try:
subprocess.run(["ffmpeg", "-version"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
return True
except FileNotFoundError:
st.error(f"FFmpeg executable not found. It should be installed via 'packages.txt'. Please check your packages.txt file and ensure it contains 'ffmpeg'.")
return False
except subprocess.CalledProcessError as e:
st.error(f"FFmpeg returned an error during version check: {e}. This might indicate a problem with the ffmpeg installation via packages.txt.")
return False
except Exception as e:
st.error(f"An unexpected error occurred during FFmpeg setup: {e}. Please check your deployment logs for more details.")
return False

def extract_audio_pydub(input_path: str, output_path: str, sr: int = 16000) -> str:
"""
   Use pydub to extract mono wav @16kHz.
   Pydub will automatically find 'ffmpeg' in the system PATH.
   Returns the path to the extracted audio if successful, None otherwise.
   """
try:
AudioSegment.converter = "ffmpeg"
audio = AudioSegment.from_file(input_path)
audio = audio.set_channels(1)  # Mono
audio = audio.set_frame_rate(sr)  # 16kHz
audio.export(output_path, format="wav")
st.success(f"Audio extracted successfully to {output_path}")
return output_path
except Exception as e:
st.error(f"Error extracting audio with pydub (FFmpeg issue?): {e}. Please check your video file format and ensure 'ffmpeg' is installed correctly via packages.txt.")
return None

# ----------------------------
# Streamlit App UI
# ----------------------------
st.set_page_config(layout="wide", page_title="Universal Subtitle Generator")

st.title("Universal Subtitle Generator üé¨")
st.markdown("""
   Upload a video file, and this app will:
   1. Extract audio using FFmpeg (via pydub).
   2. Transcribe the audio using the Faster-Whisper model with word-level timestamps (in its original language).
   3. **(Optional) Translate the transcribed text to English using OpenAI API** (requires API key).
   4. Generate an SRT subtitle file, intelligently segmenting lines based on punctuation, pauses, and configurable duration/character limits.
""")

# Sidebar for configuration
st.sidebar.header("Configuration")
model_size = st.sidebar.selectbox("Choose Whisper Model Size", MODEL_SIZES, index=MODEL_SIZES.index(DEFAULT_MODEL_SIZE))

# Maximum subtitle duration slider
bucket_seconds = st.sidebar.slider(
"Maximum Subtitle Duration (seconds)", 
min_value=1, 
max_value=10, 
value=DEFAULT_BUCKET_SECONDS,
help="Sets the maximum duration for a single subtitle entry. Shorter durations mean more frequent subtitle changes."
)

# Minimum subtitle duration slider
min_duration_seconds = st.sidebar.slider(
"Minimum Subtitle Duration (seconds)", 
min_value=0.1, 
max_value=2.0, 
value=DEFAULT_MIN_SUBTITLE_DURATION, 
step=0.1, 
help="Ensures each subtitle is displayed for at least this minimum duration. Prevents very short, flickering subtitles."
)

# Max Characters Per Subtitle Line slider
max_chars = st.sidebar.slider(
"Max Characters Per Subtitle Line", 
min_value=20, 
max_value=100, 
value=MAX_CHARS_PER_SUB,
help="Sets the maximum number of characters allowed on a single line of subtitle text. Text exceeding this limit will be wrapped to the next line."
)

# Translation Toggle and Options
st.sidebar.markdown("---")
st.sidebar.header("Translation Options (OpenAI API)")
enable_translation = st.sidebar.checkbox("Enable Translation to English", value=False,
help="If checked, the transcribed text will be translated to English using OpenAI's API. Requires an OpenAI API key in Streamlit secrets.")

uploaded_file = st.file_uploader("Upload a video file (MP4, MOV, MKV, etc.)", type=["mp4", "mov", "mkv", "avi", "webm"])

if uploaded_file is not None:
st.video(uploaded_file)

with tempfile.TemporaryDirectory() as tmpdir:
temp_video_path = os.path.join(tmpdir, uploaded_file.name)
temp_audio_path = os.path.join(tmpdir, f"{os.path.splitext(uploaded_file.name)[0]}.wav")

with open(temp_video_path, "wb") as f:
f.write(uploaded_file.getbuffer())
st.success(f"Video saved temporarily: {temp_video_path}")

with st.spinner("Checking FFmpeg setup..."):
if not ensure_ffmpeg_access():
st.error("Cannot proceed without a working FFmpeg setup. Please resolve the FFmpeg issue.")
st.stop()

with st.spinner("Extracting audio from video..."):
extracted_audio_path = extract_audio_pydub(temp_video_path, temp_audio_path)
if extracted_audio_path is None: 
st.error("Audio extraction failed. Please check the video file format and ensure FFmpeg is functioning correctly.")
st.stop()

with st.spinner(f"Loading Faster-Whisper model: {model_size} ..."):
model = load_model(model_size=model_size)

with st.spinner("Transcribing (word timestamps enabled)... This may take a while depending on audio length and model size."):
words, detected_lang = transcribe_words(_model=model, audio_path=extracted_audio_path, lang=None) 
if not words:
st.error("No words were detected. Please try a clearer audio or a different model size.")
st.stop()

with st.spinner("Building SRT with intelligent segmentation..."):
srt_blocks = bucket_words_by_duration(
words, 
bucket_seconds=bucket_seconds, 
max_chars_per_sub=max_chars, 
min_subtitle_duration=min_duration_seconds
)

final_srt_text = ""
download_filename = os.path.splitext(uploaded_file.name)[0]

if enable_translation:
st.info(f"Translating subtitles from {detected_lang.upper()} to English using OpenAI API...")
translated_srt_blocks = []

# OpenAI API calls can be slow, especially for many small requests.
# It's better to batch them if possible, but for simplicity, we translate block by block here.
# Consider a progress bar here for long videos.
progress_text = "Translation in progress. Please wait."
my_bar = st.progress(0, text=progress_text)

for i, block in enumerate(srt_blocks):
translated_text = translate_text_openai(block["text"], target_language="English", source_language=detected_lang)
translated_block = block.copy()
translated_block["text"] = translated_text
translated_srt_blocks.append(translated_block)
my_bar.progress((i + 1) / len(srt_blocks), text=progress_text)

my_bar.empty() # Remove progress bar when done
final_srt_text = assemble_srt_text(translated_srt_blocks)
download_filename += "_english_sub.srt"
st.success("Translation complete!")
else:
final_srt_text = assemble_srt_text(srt_blocks)
download_filename += f"_{detected_lang}_sub.srt"

st.success("Done!")
st.subheader("Preview (SRT)")
st.text_area("SRT Content", final_srt_text, height=320)

st.download_button(
"Download SRT",
data=final_srt_text.encode("utf-8"),
file_name=download_filename,
mime="text/plain"
)
